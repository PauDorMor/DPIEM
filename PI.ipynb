{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PauDorMor/DPIEM/blob/main/PI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DPIEM\n",
        "En el siguiente codigo se muestra y se trata de explicar el funcionamiento completo del codigo, el cual tiene la finalidad de poder clasificar e identificar EM disponibles en un espacio hospitalario"
      ],
      "metadata": {
        "id": "RQzv-dxqIGPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importe de librerias"
      ],
      "metadata": {
        "id": "oTnosLNCIamE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgODsRXR79dE"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import LinearSVC\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from scipy.special import softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "67c3e95b",
        "outputId": "ed3aad3a-8a29-49d0-93bd-63046d087998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.12/dist-packages (1.2.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.11.12)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Requirement already satisfied: pi-heif<2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n",
            "Requirement already satisfied: pillow-avif-plugin<2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n"
          ]
        }
      ],
      "source": [
        "pip install roboflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracción de imagenes y carpetas de la base de datos"
      ],
      "metadata": {
        "id": "o1XbK3QxRJkC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9edc3426",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e556e524-f8d4-40ac-ff40-0a500090e60a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dataset downloaded to: /content/medical-equipment-detection-3\n"
          ]
        }
      ],
      "source": [
        "# Import the Roboflow library\n",
        "from roboflow import Roboflow\n",
        "import os\n",
        "\n",
        "# NOTE: You will need to replace these with your actual Roboflow API key and project details.\n",
        "# You can find your API key in your Roboflow account settings.\n",
        "# The project details (workspace, project, version) are available on your dataset's page.\n",
        "\n",
        "\n",
        "ROBOFLOW_API_KEY = \"ysXFekfRklPYfXPY5kQa\"\n",
        "ROBOFLOW_WORKSPACE = \"pau-dm-quidt\"\n",
        "ROBOFLOW_PROJECT = \"medical-equipment-detection-t5ltj\"\n",
        "ROBOFLOW_VERSION = 3 # or the version number you want to download\n",
        "\n",
        "# Initialize Roboflow\n",
        "rf = Roboflow(api_key=\"ysXFekfRklPYfXPY5kQa\")\n",
        "project = rf.workspace(\"pau-dm-quidt\").project(\"medical-equipment-detection-t5ltj\")\n",
        "\n",
        "# Download the dataset\n",
        "# Specify the format (e.g., 'yolov5', 'tensorflow', 'voc', 'coco') that matches your needs.\n",
        "# For example, if you want to use it with TensorFlow/Keras, 'tensorflow' might be appropriate.\n",
        "dataset = project.version(ROBOFLOW_VERSION).download(\"tfrecord\")\n",
        "\n",
        "print(f\"Dataset downloaded to: {dataset.location}\")\n",
        "\n",
        "# You can then access the dataset details:\n",
        "# print(dataset.splits)\n",
        "# print(dataset.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dfd46dd",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810ad512-4c81-4d2d-ab8c-bfe7b676ddee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the Roboflow dataset directory '/content/medical-equipment-detection-3':\n",
            "medical-equipment-detection-3/\n",
            "    README.dataset.txt\n",
            "    README.roboflow.txt\n",
            "    valid/\n",
            "        medical-euipment-detection.tfrecord\n",
            "        medical-euipment-detection_label_map.pbtxt\n",
            "    train/\n",
            "        medical-euipment-detection.tfrecord\n",
            "        medical-euipment-detection_label_map.pbtxt\n",
            "    test/\n",
            "        medical-euipment-detection.tfrecord\n",
            "        medical-euipment-detection_label_map.pbtxt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assuming 'dataset.location' holds the path to your downloaded Roboflow dataset\n",
        "roboflow_dataset_path = dataset.location\n",
        "\n",
        "print(f\"Contents of the Roboflow dataset directory '{roboflow_dataset_path}':\")\n",
        "for root, dirs, files in os.walk(roboflow_dataset_path):\n",
        "    level = root.replace(roboflow_dataset_path, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files:\n",
        "        print(f'{subindent}{f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8851c084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7e942a-79b6-495e-91bc-da7a72e162b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Unique Class Names: ['Anesthesia Machine', 'BI', 'C-arm', 'Desf', 'ECG', 'ESU', 'EtO', 'MSV', 'US', 'VM', 'X-Ray']\n",
            "Number of Unique Classes: 11\n",
            "Label ID from TFRecord to 0-indexed Unique Class ID Mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10}\n",
            "0-indexed Unique Class ID to Name Mapping: {0: 'Anesthesia Machine', 1: 'BI', 2: 'C-arm', 3: 'Desf', 4: 'ECG', 5: 'ESU', 6: 'EtO', 7: 'MSV', 8: 'US', 9: 'VM', 10: 'X-Ray'}\n"
          ]
        }
      ],
      "source": [
        "# Assuming Ruta_general is already defined from previous cells\n",
        "Ruta_general = '/content/medical-equipment-detection-3'\n",
        "\n",
        "# Path to the label map file (using the train split as an ejemplo)\n",
        "train_label_map_dir = os.path.join(Ruta_general, 'train')\n",
        "label_map_filename = None\n",
        "\n",
        "# Dynamically find the .pbtxt file in the directory\n",
        "for file in os.listdir(train_label_map_dir):\n",
        "    if file.endswith('_label_map.pbtxt'):\n",
        "        label_map_filename = file\n",
        "        break\n",
        "\n",
        "if label_map_filename is None:\n",
        "    raise FileNotFoundError(f\"Could not find a label map file in {train_label_map_dir}\")\n",
        "\n",
        "label_map_path = os.path.join(train_label_map_dir, label_map_filename)\n",
        "\n",
        "# Store all item names as they appear, and the id-to-name mapping\n",
        "raw_class_names = []\n",
        "label_id_to_raw_name = {}\n",
        "with open(label_map_path, 'r') as f:\n",
        "    content = f.read()\n",
        "    # More robust regex to find id and display_name within each item block\n",
        "    items_blocks = re.findall(r'item\\s*{([^}]*)}', content)\n",
        "    for block in items_blocks:\n",
        "        id_match = re.search(r'id:\\s*(\\d+)', block)\n",
        "        name_match = re.search(r'display_name:\\s*\"([^\"]*)\"', block)\n",
        "        if id_match and name_match:\n",
        "            item_id = id_match.group(1)\n",
        "            item_name = name_match.group(1)\n",
        "            item_name_stripped = item_name.strip()\n",
        "            label_id_to_raw_name[int(item_id)] = item_name_stripped\n",
        "            raw_class_names.append(item_name_stripped)\n",
        "\n",
        "# Create a list of unique class names and a mapping from unique name to 0-indexed integer ID\n",
        "unique_class_names = []\n",
        "seen_names = set()\n",
        "for name in raw_class_names:\n",
        "    if name not in seen_names:\n",
        "        unique_class_names.append(name)\n",
        "        seen_names.add(name)\n",
        "\n",
        "# This will be our canonical list of class names for classification\n",
        "class_names = unique_class_names\n",
        "num_unique_classes = len(class_names)\n",
        "\n",
        "# Create a mapping from label ID (from TFRecord) to our 0-indexed unique class ID\n",
        "# This assumes that if multiple label_ids map to the same raw_name, they should all map to the same unique_class_idx\n",
        "label_id_to_unique_class_idx = {}\n",
        "for label_id, raw_name in label_id_to_raw_name.items():\n",
        "    if raw_name in class_names: # Should always be true if constructed correctly\n",
        "        label_id_to_unique_class_idx[label_id] = class_names.index(raw_name)\n",
        "    else:\n",
        "        # Fallback or error handling if a raw_name is not in unique_class_names (should not happen)\n",
        "        label_id_to_unique_class_idx[label_id] = -1 # Indicate an error or unknown class\n",
        "\n",
        "print(\"Detected Unique Class Names:\", class_names)\n",
        "print(\"Number of Unique Classes:\", num_unique_classes)\n",
        "print(\"Label ID from TFRecord to 0-indexed Unique Class ID Mapping:\", label_id_to_unique_class_idx)\n",
        "print(\"0-indexed Unique Class ID to Name Mapping:\", {idx: name for idx, name in enumerate(class_names)})\n",
        "\n",
        "# Global variable for downstream use\n",
        "num_classes = num_unique_classes\n",
        "\n",
        "# Create a TensorFlow lookup table for mapping raw_label_id to unique_class_idx\n",
        "# This will replace tf.py_function for robustness within the tf.data pipeline\n",
        "keys_tensor = tf.constant(list(label_id_to_unique_class_idx.keys()), dtype=tf.int64)\n",
        "values_tensor = tf.constant(list(label_id_to_unique_class_idx.values()), dtype=tf.int64)\n",
        "initializer = tf.lookup.KeyValueTensorInitializer(keys_tensor, values_tensor)\n",
        "label_lookup_table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa6c111c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75cacda7-06ce-4dfc-ba9e-5ce09cadd1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading a few samples from the TFRecord dataset:\n",
            "  Image shape: (224, 224, 3), Unique Class ID: 8 (Class: US)\n",
            "  Image shape: (224, 224, 3), Unique Class ID: 9 (Class: VM)\n",
            "  Image shape: (224, 224, 3), Unique Class ID: 3 (Class: Desf)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Define the path to a TFRecord file (e.g., from the 'train' split)\n",
        "tfrecord_file_path = os.path.join(Ruta_general, 'train', 'medical-euipment-detection.tfrecord')\n",
        "\n",
        "# A parsing function for TFRecord examples\n",
        "def parse_tf_example(example_proto):\n",
        "    # Define the features expected in your TFRecord file\n",
        "    feature_description = {\n",
        "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
        "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "    # Renamed 'image' to 'img_tensor' to avoid shadowing the imported 'image' module\n",
        "    img_tensor = tf.image.decode_jpeg(example['image/encoded'], channels=3)\n",
        "    img_tensor = tf.image.resize(img_tensor, [224, 224])\n",
        "    img_tensor = tf.cast(img_tensor, tf.float32)\n",
        "\n",
        "    labels_tensor = example['image/object/class/label'].values\n",
        "\n",
        "    # Check if there are any labels. If not, assign a sentinel value for filtering.\n",
        "    if tf.size(labels_tensor) > 0:\n",
        "        raw_label_id = labels_tensor[0]\n",
        "        # Use the TensorFlow lookup table instead of tf.py_function\n",
        "        unique_class_idx = label_lookup_table.lookup(raw_label_id)\n",
        "    else:\n",
        "        # If no labels, assign -1 to indicate it should be filtered later\n",
        "        unique_class_idx = tf.constant(-1, dtype=tf.int64)\n",
        "\n",
        "    return img_tensor, unique_class_idx\n",
        "\n",
        "# Create a TFRecordDataset\n",
        "raw_dataset = tf.data.TFRecordDataset(tfrecord_file_path)\n",
        "\n",
        "# Map the parsing function to the dataset\n",
        "parsed_dataset_with_sentinel = raw_dataset.map(parse_tf_example)\n",
        "\n",
        "# Filter out examples that had no labels (indicated by unique_class_idx == -1)\n",
        "parsed_dataset = parsed_dataset_with_sentinel.filter(lambda image, label: tf.not_equal(label, -1))\n",
        "\n",
        "# Iterate over a few examples to see the output (for demonstration)\n",
        "print(\"\\nLoading a few samples from the TFRecord dataset:\")\n",
        "# For printing, we need the 0-indexed ID to name mapping, which can be created from class_names\n",
        "class_idx_to_name_map_for_print = {idx: name for idx, name in enumerate(class_names)}\n",
        "for image, unique_class_idx in parsed_dataset.take(3):\n",
        "    print(f\"  Image shape: {image.shape}, Unique Class ID: {unique_class_idx.numpy()} (Class: {class_idx_to_name_map_for_print.get(unique_class_idx.numpy(), 'Unknown')})\")\n",
        "\n",
        "# The `parsed_dataset` now yields (image, unique_class_idx) pairs.\n",
        "# This dataset can be used for feature extraction and model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inicio de modelo de extracción de caracteristicas"
      ],
      "metadata": {
        "id": "68JxkTySRadb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deNJ23YQdK9v"
      },
      "outputs": [],
      "source": [
        "# Model initializations for feature extraction\n",
        "# include_top=False removes the classification layers, and pooling='avg' provides a feature vector\n",
        "model_inception = InceptionResNetV2(weights='imagenet', include_top=False, pooling='avg')\n",
        "model_resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "model_vgg = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# The previous definitions of Ruta_boot, Ruta_shoe, Ruta_sandal and their usage to collect routes\n",
        "# are not compatible with the TFRecord dataset structure and have been removed.\n",
        "# Feature extraction will now directly use the parsed TFRecord dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "785aabe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf890dd-513f-48e9-add7-43edd54ab697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset prepared with batch size: 32\n",
            "Number of unique classes detected: 11\n",
            "Shape of one batch of images: (32, 224, 224, 3)\n",
            "Shape of one batch of labels: (32,)\n",
            "Example of sparse integer label: 9\n"
          ]
        }
      ],
      "source": [
        "# Define batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Function to apply VGG16 preprocessing\n",
        "def apply_vgg16_preprocessing_and_label_mapping(image, unique_class_idx):\n",
        "    image = vgg16_preprocess(image)\n",
        "    # unique_class_idx is already mapped to the 0-indexed ID in parse_tf_example\n",
        "    # So we just pass it through\n",
        "    return image, unique_class_idx\n",
        "\n",
        "# Prepare the dataset\n",
        "# Apply preprocessing, shuffle, batch, and prefetch\n",
        "# Use the parsed_dataset that now yields (image, unique_class_idx)\n",
        "train_dataset = parsed_dataset.map(apply_vgg16_preprocessing_and_label_mapping)\n",
        "\n",
        "# The labels are already 0-indexed unique class IDs, suitable for SparseCategoricalCrossentropy.\n",
        "# No need for one-hot encoding if using SparseCategoricalCrossentropy.\n",
        "\n",
        "# Shuffle, batch, and prefetch\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"Dataset prepared with batch size: {BATCH_SIZE}\")\n",
        "# num_classes is already set globally from cell 8851c084 after ensuring uniqueness\n",
        "print(f\"Number of unique classes detected: {num_classes}\")\n",
        "\n",
        "# You can iterate a batch to see the shapes\n",
        "for images, labels in train_dataset.take(1):\n",
        "    print(f\"Shape of one batch of images: {images.shape}\")\n",
        "    print(f\"Shape of one batch of labels: {labels.shape}\")\n",
        "    print(f\"Example of sparse integer label: {labels[0].numpy()}\")\n",
        "    # Verify label is within range [0, num_classes-1]\n",
        "    if num_classes > 0 and (labels[0].numpy() >= num_classes or labels[0].numpy() < 0):\n",
        "        print(f\"Warning: Label {labels[0].numpy()} is out of range for {num_classes} classes.\")\n",
        "    elif num_classes == 0:\n",
        "        print(\"Warning: No unique classes detected, labels might be invalid.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new_cell_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2acec67-3984-4f6b-e255-adbce47b29d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features from the dataset using VGG16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]WARNING:tensorflow:5 out of the last 25 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78643591bf60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "22it [09:21, 25.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction complete. Total samples: 684\n",
            "Shape of all_features: (684, 512)\n",
            "Shape of all_labels: (684,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize lists to store features and labels\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"Extracting features from the dataset using VGG16...\")\n",
        "\n",
        "# Iterate over the preprocessed dataset and extract features\n",
        "# Use 'model_vgg' (VGG16 without top layers) for feature extraction\n",
        "for images, labels in tqdm(train_dataset):\n",
        "    # Extract features using the VGG16 model (pooling='avg' already applied)\n",
        "    features = model_vgg.predict(images, verbose=0)\n",
        "\n",
        "    # Extend the lists with extracted features and labels\n",
        "    all_features.extend(features)\n",
        "    all_labels.extend(labels.numpy())\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "all_features = np.array(all_features)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "print(f\"Feature extraction complete. Total samples: {len(all_labels)}\")\n",
        "print(f\"Shape of all_features: {all_features.shape}\")\n",
        "print(f\"Shape of all_labels: {all_labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new_cell_2",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af52bc2f-769b-41a1-ca4c-37bd88f810f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating classifier...\n",
            "\n",
            "--- Fold 1 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Liblinear failed to converge, increase the number of iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8116\n",
            "Precision (macro): 0.8479\n",
            "Recall (macro): 0.7901\n",
            "F1-Score (macro): 0.7967\n",
            "Kappa: 0.7868\n",
            "\n",
            "--- Fold 2 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Liblinear failed to converge, increase the number of iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8261\n",
            "Precision (macro): 0.8552\n",
            "Recall (macro): 0.7924\n",
            "F1-Score (macro): 0.7975\n",
            "Kappa: 0.8031\n",
            "\n",
            "--- Fold 3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Liblinear failed to converge, increase the number of iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7826\n",
            "Precision (macro): 0.7514\n",
            "Recall (macro): 0.7028\n",
            "F1-Score (macro): 0.7165\n",
            "Kappa: 0.7550\n",
            "\n",
            "--- Fold 4 ---\n",
            "Accuracy: 0.7971\n",
            "Precision (macro): 0.8119\n",
            "Recall (macro): 0.8326\n",
            "F1-Score (macro): 0.8007\n",
            "Kappa: 0.7733\n",
            "\n",
            "--- Fold 5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Liblinear failed to converge, increase the number of iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8235\n",
            "Precision (macro): 0.8538\n",
            "Recall (macro): 0.8162\n",
            "F1-Score (macro): 0.8172\n",
            "Kappa: 0.8008\n",
            "\n",
            "--- Fold 6 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Liblinear failed to converge, increase the number of iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8088\n",
            "Precision (macro): 0.8285\n",
            "Recall (macro): 0.8097\n",
            "F1-Score (macro): 0.8153\n",
            "Kappa: 0.7845\n",
            "\n",
            "--- Fold 7 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Liblinear failed to converge, increase the number of iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8971\n",
            "Precision (macro): 0.9144\n",
            "Recall (macro): 0.9055\n",
            "F1-Score (macro): 0.9069\n",
            "Kappa: 0.8839\n",
            "\n",
            "--- Fold 8 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Liblinear failed to converge, increase the number of iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8529\n",
            "Precision (macro): 0.8599\n",
            "Recall (macro): 0.8787\n",
            "F1-Score (macro): 0.8529\n",
            "Kappa: 0.8351\n",
            "\n",
            "--- Fold 9 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Liblinear failed to converge, increase the number of iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8382\n",
            "Precision (macro): 0.7631\n",
            "Recall (macro): 0.7843\n",
            "F1-Score (macro): 0.7719\n",
            "Kappa: 0.8182\n",
            "\n",
            "--- Fold 10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Liblinear failed to converge, increase the number of iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8824\n",
            "Precision (macro): 0.9013\n",
            "Recall (macro): 0.8998\n",
            "F1-Score (macro): 0.8945\n",
            "Kappa: 0.8674\n",
            "\n",
            "--- Cross-Validation Results ---\n",
            "Mean Accuracy: 0.8320 (Std: 0.0346)\n",
            "Mean Precision: 0.8387 (Std: 0.0498)\n",
            "Mean Recall: 0.8212 (Std: 0.0584)\n",
            "Mean F1-Score: 0.8170 (Std: 0.0535)\n",
            "Mean Kappa: 0.8108 (Std: 0.0389)\n",
            "Classifier trained on full dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Liblinear failed to converge, increase the number of iterations.\n"
          ]
        }
      ],
      "source": [
        "print(\"Training and evaluating classifier...\")\n",
        "\n",
        "# Initialize lists to store metrics across folds\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "kappas = []\n",
        "\n",
        "# Define the classifier\n",
        "# Using LinearSVC as it's often a strong baseline for dense features\n",
        "clf = LinearSVC(random_state=42, dual=True, max_iter=2000)\n",
        "# Alternatively, for multi-class, LogisticRegression is also good:\n",
        "# clf = LogisticRegression(random_state=42, multi_class='ovr', solver='liblinear', max_iter=1000)\n",
        "# Or RandomForestClassifier\n",
        "# clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(all_features, all_labels)):\n",
        "    print(f\"\\n--- Fold {fold+1} ---\")\n",
        "    X_train, X_test = all_features[train_index], all_features[test_index]\n",
        "    y_train, y_test = all_labels[train_index], all_labels[test_index]\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "    precision = metrics.precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    recall = metrics.recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    f1 = metrics.f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    kappa = metrics.cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1s.append(f1)\n",
        "    kappas.append(kappa)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision (macro): {precision:.4f}\")\n",
        "    print(f\"Recall (macro): {recall:.4f}\")\n",
        "    print(f\"F1-Score (macro): {f1:.4f}\")\n",
        "    print(f\"Kappa: {kappa:.4f}\")\n",
        "\n",
        "# Summarize results\n",
        "print(\"\\n--- Cross-Validation Results ---\")\n",
        "print(f\"Mean Accuracy: {np.mean(accuracies):.4f} (Std: {np.std(accuracies):.4f})\")\n",
        "print(f\"Mean Precision: {np.mean(precisions):.4f} (Std: {np.std(precisions):.4f})\")\n",
        "print(f\"Mean Recall: {np.mean(recalls):.4f} (Std: {np.std(recalls):.4f})\")\n",
        "print(f\"Mean F1-Score: {np.mean(f1s):.4f} (Std: {np.std(f1s):.4f})\")\n",
        "print(f\"Mean Kappa: {np.mean(kappas):.4f} (Std: {np.std(kappas):.4f})\")\n",
        "\n",
        "# Re-train classifier on the full dataset for deployment\n",
        "final_clf = LinearSVC(random_state=42, dual=True, max_iter=2000)\n",
        "final_clf.fit(all_features, all_labels)\n",
        "print(\"Classifier trained on full dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new_cell_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7904b53f-bd9b-4269-b408-bc4a13b44b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier saved to: /content/drive/MyDrive/A - FPDI/medical_equipment_classifier.joblib\n"
          ]
        }
      ],
      "source": [
        "# Define the path to save the classifier\n",
        "classifier_save_path = '/content/drive/MyDrive/A - FPDI/medical_equipment_classifier.joblib'\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "output_dir = os.path.dirname(classifier_save_path)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Save the trained classifier\n",
        "joblib.dump(final_clf, classifier_save_path)\n",
        "print(f\"Classifier saved to: {classifier_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Puesta en marcha del modelo"
      ],
      "metadata": {
        "id": "oWLJQgfGRoPQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "new_cell_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe09bd4-a2f2-4c16-dce5-4786e62d97da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier loaded from: /content/drive/MyDrive/A - FPDI/medical_equipment_classifier.joblib\n",
            "Extracting features for image: /content/Radiopaco.png\n",
            "Predicted Class ID: 4, Predicted Class Name: ECG\n",
            "Prediction complete.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained VGG16 model for feature extraction\n",
        "# Ensure it's the same configuration as used for training (include_top=False, pooling='avg')\n",
        "model_vgg_inference = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# Load the trained classifier\n",
        "classifier_load_path = '/content/drive/MyDrive/A - FPDI/medical_equipment_classifier.joblib'\n",
        "loaded_clf = joblib.load(classifier_load_path)\n",
        "print(f\"Classifier loaded from: {classifier_load_path}\")\n",
        "\n",
        "# Define the path to the new image for prediction\n",
        "# You can change this to any image you want to classify\n",
        "new_image_path = '/content/Radiopaco.png'\n",
        "\n",
        "# Function to preprocess an image and extract features\n",
        "def extract_features_single_image(img_path, feature_extractor_model, preprocess_func):\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
        "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0) # Add batch dimension\n",
        "    x = preprocess_func(x)\n",
        "    features = feature_extractor_model.predict(x, verbose=0)\n",
        "    return features.flatten()\n",
        "\n",
        "# Extract features from the new image\n",
        "print(f\"Extracting features for image: {new_image_path}\")\n",
        "new_image_features = extract_features_single_image(new_image_path, model_vgg_inference, vgg16_preprocess)\n",
        "\n",
        "# Reshape for prediction if necessary (sklearn expects 2D array for single sample)\n",
        "new_image_features = new_image_features.reshape(1, -1)\n",
        "\n",
        "# Make a prediction\n",
        "predicted_label_id = loaded_clf.predict(new_image_features)[0]\n",
        "\n",
        "# Retrieve the class name using the global class_names list\n",
        "# Ensure class_names is available globally from the label map parsing step\n",
        "if 0 <= predicted_label_id < len(class_names):\n",
        "    predicted_class_name = class_names[predicted_label_id]\n",
        "    print(f\"Predicted Class ID: {predicted_label_id}, Predicted Class Name: {predicted_class_name}\")\n",
        "else:\n",
        "    print(f\"Predicted Class ID: {predicted_label_id}, Class name not found or invalid ID.\")\n",
        "\n",
        "print(\"Prediction complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "b8796d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5383cbad-0a9f-46cb-8d40-d09d79d1239b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier saved to: /content/drive/MyDrive/A - FPDI/medical_equipment_classifier_v3.joblib\n"
          ]
        }
      ],
      "source": [
        "# Define the path to save the classifier\n",
        "classifier_save_path = '/content/drive/MyDrive/A - FPDI/medical_equipment_classifier_v3.joblib' # Changed path to avoid overwriting image\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "output_dir = os.path.dirname(classifier_save_path)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Save the trained classifier\n",
        "joblib.dump(final_clf, classifier_save_path)\n",
        "print(f\"Classifier saved to: {classifier_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "dc90d18a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77ca418-38ce-4b62-e659-6e4d335f4a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier loaded from: /content/drive/MyDrive/A - FPDI/medical_equipment_classifier_v3.joblib\n",
            "Extracting features for image: /content/desf.webp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 25 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78643ff965c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class ID: 3, Predicted Class Name: Desf\n",
            "Confidence (approximate): 72.66%\n",
            "Prediction complete.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained VGG16 model for feature extraction\n",
        "# Ensure it's the same configuration as used for training (include_top=False, pooling='avg')\n",
        "model_vgg_inference = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# Load the trained classifier\n",
        "classifier_load_path = '/content/drive/MyDrive/A - FPDI/medical_equipment_classifier_v3.joblib' # Ensure this matches the new save path\n",
        "loaded_clf = joblib.load(classifier_load_path)\n",
        "print(f\"Classifier loaded from: {classifier_load_path}\")\n",
        "\n",
        "# Define the path to the new image for prediction\n",
        "# YOU MUST CHANGE THIS TO THE ACTUAL PATH OF YOUR IMAGE FILE\n",
        "new_image_path = '/content/desf.webp' # <--- UPDATE THIS LINE\n",
        "\n",
        "# Function to preprocess an image and extract features\n",
        "def extract_features_single_image(img_path, feature_extractor_model, preprocess_func):\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
        "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0) # Add batch dimension\n",
        "    x = preprocess_func(x)\n",
        "    features = feature_extractor_model.predict(x, verbose=0)\n",
        "    return features.flatten()\n",
        "\n",
        "# Extract features from the new image\n",
        "print(f\"Extracting features for image: {new_image_path}\")\n",
        "new_image_features = extract_features_single_image(new_image_path, model_vgg_inference, vgg16_preprocess)\n",
        "\n",
        "# Reshape for prediction if necessary (sklearn expects 2D array for single sample)\n",
        "new_image_features = new_image_features.reshape(1, -1)\n",
        "\n",
        "# Make a prediction\n",
        "predicted_label_id = loaded_clf.predict(new_image_features)[0]\n",
        "\n",
        "# Get decision scores for all classes\n",
        "decision_scores = loaded_clf.decision_function(new_image_features)[0] # Get scores for the single sample\n",
        "\n",
        "# Convert decision scores to \"probabilities\" using softmax\n",
        "# This gives a confidence-like score, but it's not a true probability from SVC\n",
        "confidences = softmax(decision_scores)\n",
        "\n",
        "# Get the confidence for the predicted class\n",
        "predicted_confidence = confidences[predicted_label_id] * 100 # Convert to percentage\n",
        "\n",
        "# Retrieve the class name using the global class_names list\n",
        "# Ensure class_names is available globally from the label map parsing step\n",
        "if 0 <= predicted_label_id < len(class_names):\n",
        "    predicted_class_name = class_names[predicted_label_id]\n",
        "    print(f\"Predicted Class ID: {predicted_label_id}, Predicted Class Name: {predicted_class_name}\")\n",
        "    print(f\"Confidence (approximate): {predicted_confidence:.2f}%\")\n",
        "else:\n",
        "    print(f\"Predicted Class ID: {predicted_label_id}, Class name not found or invalid ID.\")\n",
        "\n",
        "print(\"Prediction complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d516992e"
      },
      "source": [
        "### Investigating Label Distribution\n",
        "\n",
        "Before proceeding with cross-validation, let's examine the distribution of classes in your dataset. This will help understand why `StratifiedKFold` might be failing, especially if some classes have very few samples, making it impossible to distribute them across all folds evenly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn2vDFITqbzY"
      },
      "source": [
        "Importar libreria desde Roboflow, dataset de 1029 imagenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90505b8a"
      },
      "source": [
        "Once you see the output of the directory structure, you'll likely find folders like `train`, `valid`, and `test`, each containing `.tfrecord` files (e.g., `_annotations.tfrecord`). To load these, you'll typically use TensorFlow's `tf.data.TFRecordDataset` API, which is designed to parse these files and extract images and labels. You would then need to adapt your feature extraction and classification pipeline to work with this new data loading mechanism, rather than directly reading image files from individual category folders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af3c8f3c"
      },
      "source": [
        "### 1. Extracting Class Names from the Label Map\n",
        "\n",
        "First, we'll parse the `medical-euipment-detection_label_map.pbtxt` file to get the mapping of numerical IDs to class names. This is crucial for understanding what each label in the TFRecord files refers to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64c1b0b9"
      },
      "source": [
        "### 2. Reading and Parsing TFRecord Files\n",
        "\n",
        "Now that we have the class names, the next step is to read the `.tfrecord` files. This involves using `tf.data.TFRecordDataset` and a parsing function to extract images and their corresponding label IDs from each record. Below is an example of how you might set up a basic parsing function and load a TFRecord file.\n",
        "\n",
        "**Note:** The exact feature names within the `feature_description` (e.g., `'image/encoded'`, `'image/object/class/label'`) can vary based on how the TFRecord was generated. You might need to inspect one of your `.tfrecord` files or refer to the Roboflow dataset's documentation for the precise feature keys."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef6c48f2"
      },
      "source": [
        "After downloading, the dataset will be available in the specified `dataset.location` directory. You can then load images and annotations from this directory based on the format you chose (e.g., 'tensorflow', 'yolov5')."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a7d5a88"
      },
      "source": [
        "## Preparación del Dataset para Entrenamiento\n",
        "\n",
        "Ahora que hemos parseado correctamente los TFRecords y tenemos un mapeo de IDs a nombres de clases, el siguiente paso es preparar el dataset para el entrenamiento. Esto implica:\n",
        "\n",
        "1.  **Crear lotes (batches)**: Agrupar múltiples imágenes para que el modelo pueda procesarlas eficientemente.\n",
        "2.  **Aplicar preprocesamiento**: Usar la función `preprocess_input` del modelo VGG16 para escalar y normalizar los valores de los píxeles de la imagen de acuerdo a los requisitos del modelo.\n",
        "3.  **Configurar la precarga (prefetch)**: Optimizar el rendimiento de la carga de datos para que el CPU prepare el siguiente lote mientras el GPU procesa el actual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "676366d6"
      },
      "source": [
        "# Task\n",
        "Create a pipeline to classify images as \"Boot\", \"Shoe\", or \"Sandal\" using pre-trained deep learning models (VGG16, ResNet50, InceptionResNetV2) for feature extraction and a Multinomial Naive Bayes classifier for classification. The process involves:\n",
        "1.  **Initializing Libraries and Loading Models**: Set up necessary libraries and load the pre-trained VGG16, ResNet50, and InceptionResNetV2 models.\n",
        "2.  **Organizing and Labeling Data**: Collect image paths for each category (\"Boot\", \"Shoe\", \"Sandal\") from the dataset located at `/content/drive/MyDrive/A - FPDI/Shoe vs Sandal vs Boot Dataset`, and create label files for binary classification scenarios (Boot vs Shoe, Boot vs Sandal, Shoe vs Sandal).\n",
        "3.  **Extracting Features**: Implement a function to extract features from images using the VGG16 model and save these features to text files.\n",
        "4.  **Training and Evaluating the Classifier**: Read the extracted features and labels, perform stratified cross-validation, train a Multinomial Naive Bayes classifier, and evaluate its performance using metrics like accuracy, precision, recall, and F1-score.\n",
        "5.  **Saving the Classifier**: Save the trained classifier using `joblib`.\n",
        "6.  **Predicting with a New Image**: Load a new image (e.g., `/content/drive/MyDrive/A - FPDI/Bota.jpg`), extract its features using the VGG16 model, load the saved classifier, and make a prediction on the image's category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52d2bfb4"
      },
      "source": [
        "## Inicialización y Carga de Modelos\n",
        "\n",
        "### Subtask:\n",
        "Initialize necessary libraries, load pre-trained deep learning models (VGG16, ResNet50, InceptionResNetV2), and define file paths for the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f51f910a"
      },
      "source": [
        "## Finalizar Tarea\n",
        "\n",
        "### Subtask:\n",
        "Resume el flujo general del código, desde la entrada de datos hasta la salida de predicciones, resaltando los componentes principales y su interacción.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ab1b770"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   All required libraries, including `matplotlib.pyplot`, `numpy`, `cv2`, `tensorflow`, `keras`, `os`, and specific Keras application modules, were successfully imported.\n",
        "*   Pre-trained deep learning models (`VGG16`, `ResNet50`, and `InceptionResNetV2`) were confirmed to be loaded with 'imagenet' weights.\n",
        "*   File paths for the dataset, including specific sub-paths for 'Boot', 'Shoe', and 'Sandal' categories, and output directories for feature extraction, were correctly defined.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The successful initialization of libraries and loading of models forms a robust foundation for the subsequent data processing and model training steps, ensuring that all necessary tools are in place.\n",
        "*   The clear definition of file paths is crucial for organized data access and feature storage, which will streamline the workflow for feature extraction and model training.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwi3uNSCVfA3b5jtInO/Bm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}